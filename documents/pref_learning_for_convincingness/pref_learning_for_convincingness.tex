\documentclass[11pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage[fleqn]{amsmath}
\usepackage{multirow}
\usepackage{url}
\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
\usepackage[hidelinks]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\usepackage[draft]{todonotes}

% \documentclass[11pt]{article}
% \usepackage{acl2016}
% \usepackage{times}
% \usepackage{url}
% \usepackage{latexsym}
% 
% \usepackage[fleqn]{amsmath}
% \usepackage{amssymb}
% \usepackage{amstext}
% \usepackage{amsthm}
% 
% \usepackage{cite}

\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{algorithm2e}
\usepackage{array}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{url}
\usepackage{tabularx}
\usepackage{numprint}
\usepackage{multirow}

\newcommand{\bs}{\boldsymbol}  
\newcommand{\wrtd}{\mathrm{d}}

\makeatletter
\makeatother %some sort of hack related to the symbol @

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{ 
Finding Convincing Arguments using Scalable Bayesian Preference Learning
}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}
\begin{document}

\maketitle

% Points to address:
% 1. Is scalability needed when data is sparse? Yes -- for a single topic we quickly hit the 
% limits with > 500 comparisons; these pairs may be noisy rather than just small; the 
% large feature space means data is still sparse; scalabe method means that model can be
% continuously updated as new data arrives.

\begin{abstract}
%Automated methods for identifying convincing arguments can be hampered by 
%a lack of gold-standard ratings or rankings in new topics or domains.
%We address this problem by introducing a scalable Bayesian preference learning method
%for argument convincingness.
We introduce a scalable Bayesian preference learning method for identifying
convincing arguments in the absence of gold-standard ratings or rankings.
% so far, classifiers and graph ranking methods have been adapted to PL -- our approach addresses the problem without a pipeline and without 
In contrast to previous work, we avoid the need for separate approaches or pipelines
to produce training data, predict rankings and perform pairwise classification.
Although Bayesian methods are known to be effective when faced with sparse or noisy training data, 
they have not previously been used to identify convincing arguments.
One deterrent is their perceived lack of scalability, which we address by developing a 
stochastic variational inference method for Gaussian process (GP) preference learning.
We show how our method can be applied to predict argument convincingness from crowdsourced data, 
outperforming state-of-the-art methods, particularly when the data is sparse or noisy.  
We demonstrate how our Bayesian approach enables more effective active learning,
thereby reducing the amount of data required to identify convincing arguments for new users and domains.
While word embeddings are principally used with neural networks, our results show that word embeddings in combination with linguistic features also benefit GPs when predicting
argument convincingness.

\todo{Two options for the paper's main claim (at the moment it is more toward number 1): (1) We provide a scalable Bayesian method that improves preference learning with sparse/noisy data (2) We show state of the art performance for argumentation by developing a scalable Bayesian preference learning method. 
Notes from other papers in source comments:
%From reece 2011 -- data fusion task is put first; main claim is not more general than the application domain of data fusion:
%"The complex target of interest in this paper is the ‘pattern of life’ associated with a counter-insurgency (COIN) scenario involving vehicle movements between specific buildings in a fictitious town... The focus of this paper is the formulation and proof-of- principle testing of a mathematical framework for representing and fusing heterogeneous data sources to provide inference in
%support of intelligence analysts."
% Look at structure of intro to "A Bayesian Approach to Unsupervised Semantic Role Induction". The application domain is front-and-centre here. 
Concerns about the first claim -- currently it is phrased in a way that is too general, e.g. "We demonstrate how our Bayesian approach enables more effective active learning"; it would have to be more specific to the scalable method presented, e.g. "...from sparse and noisy preferences with large real-world NLP datasets". To emphasise claim 2: "...of argument convincingess from small numbers of judgements provided by human annotators". 
Options: (1) claim 1 should be played down completely so it can be made into a separate ML paper; (2) claim 2 becomes focus but claim 1 is rephrased to specifically back it up; (3) claim 1 is the main focus but need a lot more background on the use cases in NLP and prior work on Bayesian methods in NLP + sell as new tool for NLP.
}
\end{abstract}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
%\IEEEpeerreviewmaketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{sections/intro}
\input{sections/method}
\input{sections/experiments}
\input{sections/discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% use section* for acknowledgment
\section*{Acknowledgments}

\cleardoublepage

% \addcontentsline{toc}{chapter}{Bibliography}
%\bibliographystyle{apalike}
\bibliographystyle{acl2012}
\bibliography{simpson_pref_learning_for_convincingness}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
