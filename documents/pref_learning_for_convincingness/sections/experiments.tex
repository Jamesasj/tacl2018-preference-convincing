\section{Experiments}\label{sec:expts}

\subsection{Datasets}
\begin{table*}[h]
\small
  \begin{tabularx}{\textwidth}{ p{2.0cm} | p{0.6cm} p{1.2cm} p{1.2cm} X }
  Dataset & Pairs & Arguments & Undecided & Dataset properties \\\hline\hline
  Toy Datasets & 4-13 & 4-5 & 0-9 & Synthetic pairwise labels
  \newline Arguments sampled at random from UKPConvArgStrict\\  
  \hline\emph{UKPConvArg-Strict} &
  11642 &
  1052 & 
  0 &
  Combine crowdsourced pairwise labels with MACE \newline
  Gold labels are $\ge 95\%$ most confident MACE labels \newline
  Discard arguments marked as equally convincing \newline
  Discard conflicting preferences \\
  \hline\emph{UKPConvArg-Rank} &
  16081 &
  1052 &
  3289 &
  Combine crowdsourced pairwise labels with MACE \newline
  Gold labels are $\ge 95\%$ most confident MACE labels \newline
  PageRank run on each topic to produce gold rankings \\  
  \hline\emph{UKPConvArg-CrowdSample} &
  16927 & 
  1052 &
  3698 &
  One original crowdsourced label per pair\newline
  PageRank run on each topic to produce gold rankings \newline
  Labels for evaluation from UKPConvArgStrict/UKPConvArgRank
  \end{tabularx}
  \caption{\label{tab:expt_data} Summary of datasets, showing the different steps used to produce each Internet argument dataset.}
\end{table*}
\begin{figure*}
\subfloat[no cycle]{
  \includegraphics[width=.46\columnwidth, clip=True, trim=60 30 20 30]{figures/cycles_demo/no_cycle/arggraph_arg_graph}
}
\subfloat[single cycle]{
  \includegraphics[width=.5\columnwidth, clip=True, trim=30 30 20 30]{figures/cycles_demo/simple_cycle/arggraph_arg_graph}
}
\subfloat[double cycle]{
  \includegraphics[width=.5\columnwidth, clip=True, trim=30 30 20 30]{figures/cycles_demo/double_cycle/arggraph_arg_graph}
}
\subfloat[no cycle + 9 undecided prefs.]{
  \includegraphics[width=.5\columnwidth, clip=True, trim=30 30 20 30]{figures/cycles_demo/undecided/arggraph_arg_graph}
}
\caption{Argument preference graphs for each scenario. Arrows point to the preferred argument.}
\label{fig:arg_graph}
\end{figure*}
We first use toy datasets to illustrate the behaviour of several different methods (described below).
Then, 
we analyse the scalability and performance our approach on datasets provided by Habernal and Gurevych~\shortcite{habernal2016argument},
which contain pairwise labels for arguments taken from online discussion forums.
Labels can have a value of $0$, meaning the annotator found the second argument in the pair more convincing,
$1$ if the annotator was undecided, or $2$ if the first argument was more convincing.
To test different scenarios, different pre-processing steps were used to produce the
three \emph{UKPConvArg*} datasets shown in Table \ref{tab:expt_data}.
For these datasets we perform 32-fold cross validation, using 31 folds for training and one for testing. 
Each fold corresponds to one of 16 controversial topics, and one of two stances for that topic.
\emph{UKPConvArgStrict} and \emph{UKPConvArgRank} test performance with noise-free labelled data,
while \emph{UKPConvArgCrowdSample} is used to evaluate performance with noisy crowdsourced data 
including conflicts and undecided labels, and to test the suitability of our method for active learning
to address the cold-start problem in new domains with no labelled data.
 
\subsection{Method Comparison}

\begin{figure*}
\centering
\subfloat[no cycle]{
  \includegraphics[width=.45\columnwidth, clip=True, trim=20 5 10 22]{figures/cycles_demo/no_cycle/PageRank_scores}
}
\subfloat[single cycle]{
  \includegraphics[width=.45\columnwidth, clip=True, trim=20 5 10 22]{figures/cycles_demo/simple_cycle/PageRank_scores}
}
\subfloat[double cycle]{
  \includegraphics[width=.45\columnwidth, clip=True, trim=20 5 10 22]{figures/cycles_demo/double_cycle/PageRank_scores}
}
\subfloat[cycle with 9 undecided]{
  \includegraphics[width=.45\columnwidth, clip=True, trim=20 5 10 22]{figures/cycles_demo/undecided/PageRank_scores}
  \label{fig:ugraph}
}
\caption{Mean scores over 25 repeats. Bars for GPPL show standard deviation of convincingness function posterior.}
\label{fig:scores}
\end{figure*}
Our two basic tasks are \emph{ranking} arguments by convincingness and  
\emph{classification} of pairwise labels for pairs of arguments, i.e. predicting which argument is more convincing. 
We compare our scalable Gaussian process preference learning method (\emph{GPPL}) against 
an SVM approach with radial basis function kernels, 
and a bi-directional long short-term memory network (BiLSTM),
with $64$ output nodes in the core LSTM layer. 
Both methods were tested by Habernal and Gurevych~\shortcite{habernal2016argument} and are available in our software repository.
For both the classification and ranking tasks, GPPL is trained using the pairwise labels for the training folds.
We rank arguments by their expected convincingness, $\mathbb{E}[f(\mathbf{x}_n)]\approx \hat{f}_n]$ for each argument $i$ 
with feature vector $\mathbf{x}_n$, under the approximate posterior $q(\bs f)$
output by our SVI algorithm.
Classification probabilities are obtained by substituting values of $\hat{f}_n$ for corresponding $f(i)$ and $f(j)$ terms in Equation \ref{eq:plphi}.
To apply SVM and BiLSTM to the classification task, we concatenate the feature vectors of each pair of arguments in the training and test sets, and train on the pairwise labels.
For ranking, PageRank is first applied to arguments in the training folds to obtain gold-standard scores from the pairwise labels. SVM and BiLSTM regression models are then trained using the PageRank scores.

As a Bayesian alternative to GPPL, 
we test a Gaussian process classifier (\emph{GPC}) for the classification task 
by concatenating the feature vectors of arguments in the same way as the SVM classifier.
We also evaluate a non-Bayesian approach that uses the same pairwise likelihood as GPPL
(Equation \ref{eq:plphi}) to infer function values, 
but uses them to train an SVM regression model instead of a GP (\emph{PL+SVR}).

We use two sets of input features. The \emph{ling} feature set contains $32010$ linguistic features,  
including unigrams, bigrams, ratios and counts of different parts-of-speech and verb forms,
dependency tree depth, ratio of exclamation or quotation marks, 
counts of several named entity types, POS n-grams,
presence of dependency tree production rules, readability measures,
sentiment scores, spell-checking, and word counts.
The \emph{Glove} features are word embeddings with 300 dimensions. Both of these feature sets were
developed by Habernal and Gurevych~\shortcite{habernal2016argument}.
%As word embeddings may contain complementary semantic information to linguistic features, we evaluate with each feature set and a 
We also evaluate a combination of both feature sets, \emph{ling + Glove}.
To create a single embedding vector per argument as input for GPPL,
we take the mean of individual word embeddings for tokens in the argument.
We also tested skip-thoughts~\cite{kiros2015skip} and Siamese-CBOW~\cite{kenter2016siamesecbow} 
with GPPL on UKPConvArgStrict and UKPConvArgRank, both with MLII optimisation and the median heuristic,
 both alone and combined with \emph{ling}. 
However, we found that mean Glove embeddings produced substantially better performance in all tests.
When running BiLSTM with \emph{ling} features, we include an additional dense layer with $64$ nodes. 

We set the GPPL hyper-parameters $a_0=2$ and $b_0=200$ by comparing
training set performance on UKPConvArgStrict and UKPConvArgRank against $a_0=2$, $b_0=20000$ and $a_0=2$, $b_0=2$.
The chosen prior is very weakly informative, favouring a moderate level of noise in the pairwise labels.
For the kernel function, $k_d$, we tested only the 
Mat\'ern $\frac{3}{2}$ function due to its 
effectiveness across a wide range of tasks~\cite{rasmussen_gaussian_2006}.
To set length-scales, $l_d$, we compare the median heuristic (labelled ``medi.")
with MLII optimisation using the L-BFGS optimisation algorithm (``opt."). Experiment 2 shows how
the number of inducing points, $M$, can be set to trade off speed and accuracy. 
Following those results, we set $M=500$ for Experiments 3, 4 and 5 and $M=N$ for the small toy dataset in Experiment 1.

\subsection{Experiment 1: Toy Data}

\begin{figure*}
\centering
\subfloat[no cycle]{
  \includegraphics[width=.36\columnwidth, clip=True, trim=58 5 41 24]{figures/cycles_demo/no_cycle/GPPL_probas}
}
\subfloat[single cycle]{
  \includegraphics[width=.36\columnwidth, clip=True, trim=58 5 41 24]{figures/cycles_demo/simple_cycle/GPPL_probas}
}
\subfloat[double cycle]{
  \includegraphics[width=.36\columnwidth, clip=True, trim=58 5 41 24]{figures/cycles_demo/double_cycle/GPPL_probas}
}
\subfloat[cycle with 9 undecided]{
  \includegraphics[width=.42\columnwidth, clip=True, trim=58 5 10 24]{figures/cycles_demo/undecided/GPPL_probas}
}
\caption{Mean GPPL predictions over 25 repeats. Probability that the argument 
on the horizontal axis is preferred to the argument on the vertical axis.}
\label{fig:gppl_classification}
\end{figure*}
\begin{figure*}[t]
\centering
\subfloat[no cycle]{
  \includegraphics[width=.36\columnwidth, clip=True, trim=58 5 41 24]{figures/cycles_demo/no_cycle/SVM_probas} 
}
\subfloat[single cycle]{
  \includegraphics[width=.36\columnwidth, clip=True, trim=58 5 41 24]{figures/cycles_demo/simple_cycle/SVM_probas} 
}
\subfloat[double cycle]{
  \includegraphics[width=.36\columnwidth, clip=True, trim=58 5 41 24]{figures/cycles_demo/double_cycle/SVM_probas} 
}
\subfloat[cycle with 9 undecided]{
  \includegraphics[width=.42\columnwidth, clip=True, trim=58 5 8 24]{figures/cycles_demo/undecided/SVM_probas} 
}
\caption{Mean SVM predictions over 25 repeats. Probability that the argument 
on the horizontal axis is preferred to the argument on the vertical axis.}
\label{fig:svm_classification}
\end{figure*}

We use synthetic data to illustrate the different behaviour of GPPL, SVM for pairwise classification,
and PageRank for scoring arguments.
We simulate four scenarios, each of which contains arguments labelled \emph{arg0} to \emph{arg4}.  
In each scenario, we generate a set of pairwise preference labels according to the 
convincingness graphs shown in Figure \ref{fig:arg_graph}.
Each scenario is repeated 25 times: in each repeat, we select arguments at random from one fold of UKPConvArgStrict
then associate the mean Glove embeddings for these arguments with the labels arg0 to arg4. 
We train GPPL, PageRank and the SVM classifier on the preference pairs shown in each graph and
make predictions for arguments arg0 to arg4.

In the  ``no cycle" scenario, 
arg0 is preferred to both arg1 and arg2, which is reflected in the PageRank and GPPL scores in Figure \ref{fig:scores}. However, arg3 and arg4 are not connected to the rest of the graph and receive different scores with PageRank and GPPL. 
Figure \ref{fig:gppl_classification} shows how GPPL provides probabilistic classifications that are less confident for pairs that were not yet observed, e.g. arg2 $\succ$ arg4. This contrasts with Figure \ref{fig:svm_classification} which shows discrete classifications produced by SVM.

The ``single cycle" scenario shows how each method handles a cycle in the preference graph.
Both PageRank and GPPL produce equal values for the arguments in the cycle (arg0, arg1, arg2). PageRank assigns lower scores to both arg3 and arg4 than the arguments in the cycle, 
while GPPL more intuitively gives a higher score to arg3, which was preferred to arg4. 
SVM predicts that arg0 and arg1 are preferred over arg3, 
although arg0 and arg1 are in a cycle so there is no reason to prefer them. 
GPPL, in contrast,  weakly predicts that arg3 is preferred.

In the ``double cycle" scenario, PageRank and GPPL produce very different results.
Here, the argument graph shows two paths from arg2 to arg0 via arg1 or arg3, and one conflicting
preference arg2 $\succ$ arg. 
GPPL scores the arguments as if the single conflicting preference, arg2 $\succ$ arg0, 
is less important than the two parallel paths from arg2 to arg. 
In contrast, PageRank gives high scores to both arg0 and arg2.
The classifications by GPPL and SVM are similar, but GPPL produces more uncertain 
predictions than in the first scenario due to the conflict.

%``cycle with 9 undecided prefs"
Finally,  Figure \ref{fig:ugraph} shows the addition of nine undecided labels to the ``no cycle" scenario, indicated by 
undirected edges in Figure \ref{fig:arg_graph}, to simulate a case where multiple annotators labelled the pair.
%and did not all agree. 
This does not affect the PageRank scores, 
but reduces the difference in GPPL scores between arg0 and the other arguments, 
since GPPL gives the edge from arg2 to arg0 less weight due to the undecided labels. 
This is reflected in the GPPL classifications, which are less confident than in the ``no cycle" scenario.
The SVM cannot be trained using uncertain labels and therefore does not adapt to the undecided labels. 

In conclusion, GPPL appears to resolve conflicts in the preference graphs in a
more intuitive manner than PageRank, which was designed for ranking web pages by 
importance rather than preference. 
In contrast to SVM, GPPL is able to account for undecided labels to soften the latent convincingness function.

\subsection{Experiment 2: Scalability}

\begin{figure*}
\centering
\subfloat[Varying no. arguments in training set, Glove features]{
\includegraphics[width=0.55\columnwidth, clip=True, trim=12 14 14 0]{figures/scalability/num_arguments.pdf}
\label{fig:scale_N}
}
\hspace{0.1cm}
\subfloat[Varying no. ling+Glove features, GPPL, medi., M=500]{
\includegraphics[width=0.53\columnwidth, clip=True, trim=14 0 25 0]{figures/scalability/num_features_gppl.pdf}
\label{fig:scale_dims}
}
\hspace{0.1cm}
\subfloat[Varying no. ling+Glove features, long-running methods]{
\includegraphics[width=0.55\columnwidth, clip=True, trim=14 14 14 0]{figures/scalability/num_features_others.pdf}
\label{fig:scale_dims_others}
}
\caption{Runtimes for training+prediction on UKPConvArgStrict with different subsamples of data. Means over 32 runs. Note logarithmic x-axis for (b) and (c). }
\end{figure*} 
\begin{figure}[h]
\subfloat[33210 ling+Glove features]{
\includegraphics[width=0.495\columnwidth, clip=True, trim=13 0 15 0]{figures/scalability/num_inducing_32310_features.pdf}
\label{fig:scale_M_b}}
\subfloat[300 Glove features]{
\includegraphics[width=0.495\columnwidth, clip=True, trim=15 0 13 0]{figures/scalability/num_inducing_300_features.pdf}
\label{fig:scale_M_a}}
\caption{Effect of varying $M$ on accuracy and runtime (training+prediction) of GPPL for UKPConvArgStrict.  Means over 32 runs.}
\label{fig:scale_M}
\end{figure}
%In Section \ref{sec:model}, we introduced a scalable inference method (SVI) for the GPPL model and described its theoretical complexity. 
We analyse empirically the scalability of the proposed SVI method for GPPL using the UKPConvArgStrict dataset.
Figure \ref{fig:scale_M} shows the effect of varying the number of inducing points, $M$, on the overall runtime and accuracy of the method. The accuracy increases quickly with $M$, and flattens out, suggesting there is little benefit to increasing  $M$ further on this dataset. 
The runtimes 
increase with $M$,  and are much longer with 32310 features than with 300 features.
%although it is not clear that the $\mathcal{O}(M^3)$ term  to identify the relationship from the plots as the runtime is affected by several computations that scale as a function of $M$. 
The difference is due to the cost of computing the kernel, which is linear in $M$,
With only $300$ features, Figure \ref{fig:scale_M_b} appears polynomial, reflecting the $\mathcal{O}(M^3)$ term in the inference procedure. 
 
%SVI limits the computational and memory complexity of each iteration of the variational inference algorithm. However, the number of iterations
%required for the algorithm to converge is hard to analyse theoretically. 
We tested GPPL with both the SVI algorithm, with $M=100$ and $P_n=200$, and variational inference without inducing points or stochastic updates (labelled ``no SVI'') with different sizes of training dataset subsampled from UKPConvArgStrict. 
The results are shown in Figure \ref{fig:scale_N}.
For GPPL with SVI, the runtime increases very little with dataset size,
%, suggesting that the number of iterations required does not substantially increase with the training set size in this dataset. As expected, t
while the runtime with ``no SVI'' increases polynomially with training set size (both $N$ and $P$).
%, reflecting the increasing value of both the number of arguments, $N$, and the number of pairs, $P$. 
At $N=100$, the number of inducing points is $M=N$ but the SVI algorithm is still faster due to the stochastic updates with $P_n=200$ rather than $P>200$ pairs.
% used by ``no SVI''.

%Noting that GPPL runtimes are significantly lower with smaller feature sets, we tested
Figure \ref{fig:scale_dims} shows
the effect of the number of features, $D$, on runtimes.  
Runtimes for GPPL increase by a large amount with $D=32310$,
 because the SVI method computes the kernel matrix, $K_mm$, with computational complexity $\mathcal{O}(D)$. While $D$ is small, other
costs dominate.
%, but since the other key terms $M$ and $P_n$ are fixed, 
%$\mathcal{O}(D)$ computations becomes important when $D$ is large, hence the near 
%ten-fold increase in runtime from $3000$ to $32310$ features. 
We show runtimes using the MLII optimisation procedure with GPPL in Figure \ref{fig:scale_dims_others}. 
The procedure was run with a maximum of 25 iterations
and did not terminate in fewer than 25 in any of the test runs. 
This creates a similar pattern to Figure \ref{fig:scale_dims} (approximately multiples of $50$).
Owing to the long computation times required, we were unable to test the optimisation
procedure to convergence. 
%However, the results in the next section demonstrate that MLII optimisation can be 
%valuable even with a restricted number of iterations.  

%To give an idea of how our GPPL implementation scales in comparison to the other approaches, 
We include runtimes for SVM and BiLSTM in Figures \ref{fig:scale_N} and
\ref{fig:scale_dims_others} to show their runtime patterns, but note that the runtimes reflect differences in implementations and system hardware.
Both SVM and GPPL were run on an Intel i7 quad-core desktop. For SVM we used LibSVM version 3.2, which could be sped up if probability estimates were not required.
%, as they involve a costly cross-validation step. 
BiLSTM was run with Theano 0.7\footnote{\url{http://deeplearning.net/software/theano/}} on an Nvidia Tesla P100 GPU. 
We can see in Figure \ref{fig:scale_dims_others} that the runtime for BiLSTM does
not appear to increase due to the number of features, while that of SVM increases sharply with $32310$ features. 
%The sequential model of BiLSTM takes as input up to $300$ Glove features for each word of an argument, whereas GPPL and SVM use a single mean vector per argument, so the true number of inputs to BiLSTM is far greater. With $>=3000$ features, the BiLSTM runtime decreases, which may mean that the algorithm is converging faster with more data.
 In Figure \ref{fig:scale_N}, we observe the SVM runtimes increase polynomially with training set size. 
 %With these smaller training set sizes, SVM has shorter runtimes than  BiLSTM, which may have more computational overheads independent of training set size, but is slower in Figure \ref{fig:scale_dims_others} with $1052$ arguments and the same number of features.

\subsection{Experiment 3: UKPConvArgStrict and UKPConvArgRank}

\begin{table*}
\small
  \begin{tabularx}{\textwidth}{ | l | X | X | X |  X |  X |  X |  X | X | X | X |}% X | X |}
  \hline
       &\multicolumn{2}{c|}{SVM}&\multicolumn{2}{c|}{BiLSTM}&\multicolumn{3}{c|}{GPPL medi.}&GPPL opt. & GPC & PL+ SVR\\\hline
       %& GPPL+, medi. & GPPL+, opt      \\\hline
       &ling &ling +Glove &Glove &ling +Glove &ling &Glove &\multicolumn{4}{c|}{ling +Glove}\\\hline
\multicolumn{11}{| l |}{UKPConvArgStrict (pairwise classification)} \\   \hline       
Acc.:  &.78 & .79 &.76 & .77 &.78 &.71  &.79  & .80 & \textbf{.81} & .78\\%& .78 & .78     \\
AUC:   &.83 & .86 &.84 & .86 &.85 &.77  &.87  & .87 & \textbf{.89} & .85\\%& .86  &  .86    \\
CEE:   &.52 & .47 &.64 & .57 &.51 &1.12  &.47  & .51 & \textbf{.43} & .51 \\%& .69  & .69   \\
\hline \multicolumn{11}{| l |}{UKPConvArgRank (ranking)} \\   \hline
Pearson's r:      &.36 & .37 &.32 & .36 &.38 &.33  & \textbf{.45} &  .44 & - & .39 \\%& .40 &  .40   \\
Spearman's $\rho$:&.47 & .48 &.37 & .43 &.62 &.44  &.65&  \textbf{.67} & - & .63\\% & .64 &  .64   \\
Kendall's $\tau$: &.34 & .34 &.27 & .31 &.47 &.31  &.49   &  \textbf{.50} & - & .47\\% & .49 &  .49   \\
\hline
  \end{tabularx}
  \caption{Performance comparison on UKPConvArgStrict and UKPConvArgRank datasets. }
  \label{tab:clean_results}
\end{table*}
% \todo{statistical significance? paired Wilcoxon signed-rank test, GPPL ARD vs. SVM. }
% acc p-value of .043437
% auc p-value of .129870
% cee p-value of .000147. 
% Ranking: p-value of .000001
% p-value of .000001
% p-value of .000001
% 
% GPC to GPPL:
% p-value of .000012
% p-value of .000009
% p-value of .000001
% 
% GP+SVR to GPPL:
% p-value of .078799
% p-value of .056483
% p-value of .000171 --> differences mostly not significant
% p-value of .085377
% p-value of .045415
% p-value of .114077
We compare classification performance on UKPConvArgStrict  
and ranking performance on UKPConvArgRank. 
Both datasets were cleaned to remove disagreements between annotators, as stated in Table \ref{tab:expt_data}, hence can be considered to be \emph{noise-free}.
The results in Table \ref{tab:clean_results} show that when using \emph{ling} features,
GPPL produces similar accuracy and improves the area under the ROC curve (AUC) by $.02$ and cross entropy error (CEE) by $.01$.
AUC quantifies how well the predicted probabilities separate the classes,
while CEE quantifies the usefulness of the probabilities output by each method.
Much larger improvements can be seen in the ranking metrics. 
When GPPL is run with \emph{Glove}, it performs worse than
BiLSTM for classification but improves the ranking metrics. Using a combination of features improves all methods, suggesting that embeddings and linguistic features contain complementary information.

Optimising the length-scale using MLII improves classification accuracy by 1\% over the median heuristic,
and significantly improves accuracy ($p=.043$ using two-tailed Wilcoxon signed-rank test) and AUC ($p=.013$) 
over the previous state-of-the-art, SVM with linguistic features.
The differences in all of the ranking metrics between GPPL opt. and the next-best methd, SVM with \emph{ling + Glove}, 
are statistically significant, with $p=.029$ for Pearson's $r$ and $p<<<.01$ for both 
Spearman's $\rho$ and Kendall's $\tau$.
However, the cost of these improvements is that each fold required around 2 hours to compute instead of 
approximately 10 minutes on the same machine (an Intel i7 quad-core desktop) using the median heuristic. 

GPC produces the best results on the classification task (with $p < 0.01$ for all metrics compared to all other methods), 
indicating the benefits of a Bayesian approach over SVM and BiLSTM.
%and showing directly training a classifier in some cases performs better than the GPPL approach of regression through the pairwise likelihood. 
However, unlike GPPL, GPC cannot be used to rank the arguments.
%, GPPL may be more suitable in some situations. 
The results also show that PL+SVR does not reach the same performance as GPPL, 
suggesting that GPPL may benefit from the Bayesian integration of a GP with the preference likelihood. 

\subsection{Experiment 4: Conflicting and Noisy Data}

\begin{table}
\small
  \begin{tabularx}{\columnwidth}{ | l | X | X | X | X | X |}\hline
% \multicolumn{6}{|l|}{UKPConvArgAll} \\   \hline
             & SVM & Bi-LSTM &GPPL medi.        &PL+ SVR     &GPC \\\hline
\multicolumn{6}{| l |}{Classification} \\   \hline             
% Acc:     &.71 &.73  & .77        &.75       &.76 \\
% AUC:          &.81 &.81  & .84        &.82       &.86 \\
% CEE:          &.56 &.53  & .49        &.52       &.50 \\
Acc          & .70 & .73 & \textbf{.77}        &.75       &.73 \\
AUC          & .81 & .81 & .84        &.82       & \textbf{.86} \\
CEE          & .58 & .55 & \textbf{.50}     &.55       &.53 \\\hline
\multicolumn{6}{| l |}{Ranking} \\   \hline             
Pears.       & .32 & .22 & \textbf{.35}        &.31       & - \\
Spear.       & .43 & .30 & .54        & \textbf{.55}       & - \\
Kend.        & .31 & .21 & \textbf{.40}        & \textbf{.40}       & - \\
%             &SVM ling & Bi-LSTM Glove % removed these results as not needed
%Acc          &.70 &.73 
%AUC          &.81 &.80 
%CEE          &.58 &.54
%Pears.       &.18 &.26
%Spear.       &.17 &.20
%Kend.        &.12 &.13
\hline
  \end{tabularx}
  \caption{Performance comparison on the UKPConvArgCrowdSample datasets containing conflicts and noise using ling+Glove features.}
  \label{tab:noisy}
\end{table}
We use UKPConvArgCrowdSample to introduce noisy data
and conflicting pairwise labels,
to both the classification and regression tasks, to test
the hypothesis that GPPL would best handle unreliable crowdsourced data.
The evaluation uses the labels from UKPConvArgStrict and UKPConvArgRank for items in the test set.
The results in Table \ref{tab:noisy} show that all methods perform worse compared to 
Experiment 3 due to the presence of errors in the pairwise labels in UKPConvArgCrowdSample. 
Here, GPPL produces the best classification accuracy and cross-entropy error (significant with $p<<<.01$ compared to all other methods except accuracy compared to GP+SVR, for which $p=.045$), while GPC has the highest AUC. 
Compared to UKPConvArgStrict, the classification performance of GPC, SVM and BiLSTM have decreased more than that of GPPL.
%, 
% which may be  due to the much larger input space 
 %caused by concatenating the feature vectors of the items in each pair. This increases the sparsity of the training data, which may be more problematic when training using noisy labels, since there are fewer neighbouring labels in the higher-dimensional 
%feature space to correct the errors. 
%Furthermore, GPC, SVM and BiLSTM
These methods lack a mechanism to resolve conflicts in the preference graph, unlike GPPL and PL+SVR, which handle conflicts through the preference likelihood.  
PL+SVR again performs worse than GPPL on classification metrics, although its ranking performance is comparable. 
For ranking, GPPL again outperforms SVM and BiLSTM in all metrics (significant with $p<<.01$ in all cases except for SVM with Pearson's correlation).
% while for other methods it remains the same as for UKPConvArgAll. 

\subsection{Experiment 5: Active Learning}

In this experiment, we hypothesised that GPPL provides more meaningful confidence estimates than SVM or BiLSTM,
which can be used to facilitate active learning in scenarios where labelled training data is expensive
or initially unavailable.
To test this hypothesis, we simulated an active learning scenario, in which an agent 
iteratively learns a model for each fold. Initially, $N_{inc}=2$ pairs were chosen at random, then used to train the classifier. The agent then performs \emph{uncertainty sampling}~\cite{settles2010active} 
to select the $N_{inc}=2$ pairs with the least confident classifications. 
The labels for these pairs are then added to the training set and 
used to re-train the model. The process was repeated until $400$ labels had been sampled. 

The result is plotted in Figure \ref{fig:active_learning}, showing that GPPL
reaches a mean accuracy of 70\% with only 100 labels, while SVM and BiLSTM do not reach the same performance given 400 labels. 
%The accuracy of GPPL also increases by approximately 17\% given 200 labels, while SVM increases
%approximately 6\% and BiLSTM only 2\%. 
%The accuracy of GPPL increases more rapidly than BiLSTM at first, after which the 
After 100 labels, the 
performance of BiLSTM decreases. It has previously been shown ~\cite{cawley2011baseline,guyon2011results,settles2010active} that uncertainty sampling can perform poorly in some cases, leading to periods where accuracy decreases as labels are received.
Such a failure may be more likely when a model overfits to the current, small set of samples, causing it to mis-classify some data points with high confidence, meaning the mis-classified points will not be selected.  
The larger number of parameters in the BiLSTM means that it may be more prone to overfitting with small datasets than SVM or GPPL. 
The results suggest that GPPL may outperform the alternatives in cold-start scenarios where there are initially small amounts of labelled data, since the Bayesian approach reduces overfitting by accounting for parameter uncertainty.
\begin{figure}
\centering
\includegraphics[width=0.9\columnwidth,trim=13 10 14 22,clip=true]{figures/active_learning_2/test_acc}
\caption{Active learning simulation showing mean accuracy of preference pair classifications over 32 runs.}
\label{fig:active_learning}
\end{figure}

% This plot is invalid because SVM and BiLSTM are trained on the output from PageRank, which was trained with all the data!
% \begin{figure}
% \includegraphics[width=1.0\columnwidth,trim=0 0 0 23,clip=true]{figures/active_learning/test_pearson}
% \caption{Active learning simulation for the three methods showing the mean pearson correlation with the gold standard ranking over 32 runs.}
% \end{figure}

\subsection{Relevant Feature Determination}

% \todo{how much does this really show us? Could we skip this section to make more space for error analysis?
% In contrast with Lampos et al 2014, we cannot identify a small number of important features --> looks like we need
% a combination, perhaps a hierarchical feature representation.}
Finally, we show how the length-scales learned by optimising GPPL using MLII can be used to identify
informative sets of features. 
A larger length-scale causes greater smoothing, 
implying that the feature is less relevant when predicting the convincingness function
than a feature with a small length-scale. 
%In contrast, small length-scales indicate more informative features, since their
%precise value affects the latent preference function.
Figure \ref{fig:boxplot} shows the distribution of normalised length-scales for \emph{ling+Glove}
after optimising on one fold of UKPConvArgStrict. 
Due to the computation time required, the optimisation algorithm was limited to $25$ iterations, resulting in the large number of values close to $1$,
% may result from  not being able to optimise all features in the available time.
as features with larger gradients were optimised first.
The length-scales for many dimensions of the mean word embeddings were increased,
giving ratios close to $4$ times the median heuristic, 
suggesting that these dimensions may be
only very weakly informative in comparison to other dimensions of the embedding vectors. Table \ref{tab:extreme_features} shows the largest
and smallest ratios for embeddings and linguistic features. The unigram "safety" has
a very high length-scale, suggesting it is not informative and could be discarded. 
The length-scales learned using MLII could in future be used to discard irrelevant features
automatically. 
% It is possible that continuing the optimisation procedure for a larger number of steps would 
% identify large length-scales for other features that may be discarded. However, caution is 
% required to avoid overfitting to the training set during optimisation~\cite{qi2004predictive}.
\begin{figure}[h]
\includegraphics[width=\columnwidth, clip=True, trim=32 0 57 0]{figures/features/boxplot}
\caption{Distribution of length-scales for each type of feature after MLII optimisation 
on fold ``should physical education be mandatory in schools -- no". 
Values are ratios over the median heuristic value. 
Optimisation on this fold increased accuracy from 75\% to 80\%. }
\label{fig:boxplot}
\end{figure}
\begin{table}
\small
  \begin{tabularx}{\columnwidth}{l | X }
  Feature & Ratio\\
  \hline
  ProductionRule-S-$>$ADVP,NP,VP,., & .466 \nonumber\\
  Pos-ngram-PP-O-CARD & .477 \nonumber\\
  Unigram-``safer", & .640 \nonumber\\
  \hline
  Bigram-``?"-``look" & 5.672 \nonumber\\
  Unigram-``safest" & 8.673 \nonumber\\
  Unigram-``safety" & 271.190 \nonumber\\
  \hline
  Embedding-dimension-19 & .610 \nonumber\\
  \hline
  Embedding-dimension-241 & 12.093 \nonumber\\
  \end{tabularx}
  \caption{Ratios of optimised to median heuristic length-scales: largest and smallest
  ratios for linguistic features and word embeddings.}
  \label{tab:extreme_features}
\end{table}


\subsection{Error Analysis}

We compared the errors when using GPPL opt. with mean Glove embeddings
and with linguistic features. We
manually inspected the twenty-five arguments most frequently
mis-classified by GPPL \emph{ling} and correctly classified by GPPL \emph{Glove}.
We found that GPPL \emph{ling} mistakenly marked several arguments 
as less convincing when they contained grammar and spelling errors but otherwise
made a logical point. 
In contrast, arguments that did not strongly take a side and did not contain 
language errors were often marked mistakenly as more convincing.

We also examined the twenty-five arguments most frequently misclassified by GPPL \emph{Glove} but not by GPPL \emph{ling}.
Of the arguments that GPPL \emph{Glove} incorrectly marked as more convincing, 
$10$ contained multiple exclamation marks and all-caps sentences. 
Other failures were very short arguments and underrating arguments containing the term `rape'.
The analysis suggests that the different feature sets identify different aspects of convincingness.

% Notes on first paragraph:
% ling failed on:
% overrated: good grammar and structure but weak points?
% both; short and direct argument but not very thoughtful.
% both: seems well structured...
% both: seems to argue for middle ground rather than one side
% both: grammar/spelling errors but strong argument underneath?
% both: grammar/spelling errors but strong argument underneath?
% underrated: very specific example may seem off topic? 
% both: unclear points
% underrated: bad spelling but okay points
% underrated: very personal "I" language to make a good point
% mostly underrated: makes multiple points, some are quite extreme. 
% underrated: sounds quite personal "you" but strong argument.
% both: kind of doesn't try to convince strongly for one side although well written.
%  overrrated: doesn't address the main point although language is reasonable.
% mostly overrated: response to previous answer with no new points.
% underrated: contains '?' and 'you' a lot but makes a good point.
% overrated: language is reasonable, no mistakes, but doesn't clearly take a side.
% overrated: no obvious language indicators but doesn't address main point.
% underrated: lots of spelling mistakes hide the main point.
% mostly overrated: good language but doesn't really support or attack the point directly
% underrated argument with grammatical mistakes but possibly a good point?
% avoiding the argument/debate overrated
% illogical and quite emotional argument underrated
% personal/ad hom attack with 'crap' overrated -- but need to identify the username as such to know it  is a personal attack?
% obvious nonsense without proper words was overrated
%
% embeddings failed on:
%
% both: very short argument, unclear why ling features would help
% overrated: claims lack support
% overrated: spelling mistakes; doesn't address stance of the topic 
% underrated: very short; mean embedding could be an outlier?
% underrated: short
% underrated: no linguistic markers of a bad argument; unclear why embeddings a problem
%     mostly underrated; as above
% underrated: as above; perhaps the words such as 'rape' are associated in the embeddings
% with poor emotional arguments? Perhaps not the word 'rape' itself, but some of the related words in embedding space are common in bad arguments.
% underrated: no idea
% underrated: again mentions 'murder'
% mostly underrated: all caps; why underrated?
% underrated: sensible argument mentions 'personal' a lot
% underrated: very short; mentions wrong and right --> link to emotional arguments without evidence?
% overrated: very short and gives no argument; perhaps contains no bad argument embeddings but linguistic features such as word counts will show it up.
% overrated: mentions of 'dreamt'?
% underrated: no idea
% overrated: doesn't address main point; hard to see why ling would help
% underrated: no idea why; has no linguistic errors
% overrated: no idea; no idea why linguistic features help
% underrated: again maybe because of emotional topic of abortion; no linguistic problems
% overrated: very short sentence; no structure; lots of '!'
% overrated: grammatical errors, very short text that doesn't really make any point or provide any support
% overrated: nonsense detected by all caps
% overrated: very short, cannot really make a good point in that much text
% underrated: why this would beat any other argument is unclear; it's all caps and has lots of '!' too
% 
%  Problem is we don't see what they were misclassified against -- number of 
% misclassifications is too few to know if it is a general problem with that sentence.
% 
% which of these errors are resolved by combining? Pick out errors from either ling that
% were fixed with both; pick our errors from embeddings that were fixed with both.

% when comparing 'both' to 'ling', most of the errors that are made by embeddings
% no longer appear -- those with all caps and '!' have gone. Differences are few.
% seem to be that poorly written arguments continue to be mis-classified.
% with 'both' to embeddings, some of the errors that ling made have gone, e.g. including 'lol'. The remaining ones are mainly those with good superficial structure/correct grammar
% but covering emotional topics. Suggests need for better trade-off between semantics/deeper understanding and linguistic indicators such as good grammar.

% this analysis also shows whether the errors are due to epistemic uncertainty given the features (small difference in 
% similarities between false/true labelled items) or if better training data would help (larger difference between 
% false/true similarities). If GPPL handles the sparse data better, the difference should be reduced compared to SVM.
% the text below is good but the results are not really worth reporting...
%We next investigate whether the Bayesian approach is better able to handle sparsity in the training data. 
%Training data sparsity occurs when the training provides very few examples of arguments in some important regions of feature space, meaning  
% there are arguments in the test dataset that are not similar to any training arguments.
%For each fold, we computed the mean cosine similarity of each argument in the 
%test dataset to the arguments in the training dataset, which we refer to as \emph{training similarity}.
%For UKPConvArgStrict, the mean training similarity for the arguments in pairs correctly classified by GPPL (ling + Glove) was 694.63 and the mean for incorrectly classified arguments was 694.703. This suggests that training similarity is not a strong indicator of a correct
%prediction and training data sparsity does not appear to be a key source of error for GPPL
%in this dataset. In comparison, using  SVM,  
%the mean training similarity for correctly classified arguments
% was 694.72 against .     for incorrectly classified arguments, showing that 
% lower training data similarity corresponds to a more misclassified pairs. 
% This indicates that SVM was more affected by training data sparsity.
% However, the small difference in similarities indicates that sparsity was not a problem here?
% The lower similarity of correctly labelled arguments with GPPL suggests it is able to handle more outlying arguments correctly. 
%results: 
% using matern covariance to determine similarity:
% SVM: correctly classified pairs: 694.715334 (STD 3.251511), incorrectly classified pairs: 694.164449 (STD 3.809051). 
% So similarity to training data is higher for correct pairs.
% GPPL: correctly classified pairs: 694.625999 (STD 3.213874), incorrectly classified pairs: 694.703252 (STD 3.580148).
% with cosine similarity:
%SVM For all folds: mean total_sim for correctly classified pairs: .998826 (STD .021547)
%SVM For all folds: mean total_sim for incorrectly classified pairs: .998770 (STD .025875)
%GPPL For all folds: mean total_sim for correctly classified pairs: .998825 (STD .021434)
%GPPL For all folds: mean total_sim for incorrectly classified pairs: .998770 (STD .026505)
% Using max similarity rather than mean (most similar training point) gives similar results.
% So similarity to training data is actually higher for incorrect pairs; for correct pairs it is lower than for SVM. 
% Suggests that similarity to training data (not being outliers) is less important for GPPL.

To investigate the differences between our best approach, GPPL opt. \emph{ling + Glove}, 
and the previous best performer, SVM, 
we manually examined forty randomly chosen false classifications, where one of 
either  \emph{ling + Glove} or SVM was correct and the other was incorrect. 
We found that both SVM and GPPL falsely classified arguments that were either very short or long and complex, suggesting deeper semantic or structural understanding of the argument may be required. However, SVM also made mistakes
where the arguments contained few verbs.
%We were not able to identify any common source of error for cases where both 
%methods produced errors. % required deeper semantic understanding and background knowledge of the topics involved
% to evaluate the validity of claims.
% Case 1: SVM right, GPPL wrong.
% underrated, short
% overrated, maybe SVM is right because it dislikes '!'
% underrated, less common language -- perhaps not learned?
% overrated, complex language
% underrated, short argument
% ...
% overrated, irrelevant but well written
% more long arguments with fairly sophisticated language and no obvious errors.
% some cases of arguments that do not take a strong position either way
% The number of errors are small -- could be noise rather than consistent problems with GPPL.
% Some arguments with mainly topic-specific words were misclassified by GPPL but not SVM.
%notable that there are lots of errors on the singapore topics
% Case 2: GPPL right, SVM wrong
% short arguments with few verbs
% long arguments with no obvious language errors and no emotional arguments.
% notable that lots of errors on the school uniform topics

We also compared the rankings produced by GPPL opt. (ling+Glove), 
and SVM on UKPConvArgRank by examining the 20 largest deviations from the 
gold standard rank for each method. Arguments underrated by SVM and not GPPL often 
contained exclamation marks or common spelling errors (likely due to unigram or bigram features).
GPPL underrated short arguments with the ngrams ``I think", ``why?", and
``don't know", which were used as part of a rhetorical question
rather than to state that the author was uncertain or uninformed.
These cases may not be distinguishable by a GP given only \emph{ling + Glove} features.
% Case 1: GPPL better, SVM worse
% svm underrated some arguments containing !, thankyou, lots of 'if'
% underrated: long argument, common spelling errors (unigrams usually correlated with bad arguments?), India, long and complex arguments, short arguments
% overrated: ('because' could be either over or underrated), mention of another website, 
% Hard to see pattern here because the arguments are very varied in length, grammatical standard; most are not using emotive terms.

% Case 2: SVM better, GPPL worse
% mistakes by GPPL: several short arguments containing "i think"... perhaps leading to a more neutral score than the other features suggest?
% 

% Case 3: GPPL's worst problems (including mistakes by SVM)
% an argument with lots of caps in was underrated
% argument with lots of short sentences (sometimes few verbs) is underrated. 
% underrated: "why?"
% underrated: "don't know" is a big indicator. These two suggest that the model needs a way to determine whether the "don't know" was used in a way that undermines the argument strength (admission of a lack of knowledge) or as a rhetorical device to say that an opposing viewpoint is nonsensical ("don't know why they would do x"). 

% Entropy for SVM correct/incorrect labels: .187555, 1.583709
% Entropy for GPPL correct/incorrect labels: .128872, 2.443407
% Shows better division between certain and uncertain predictions
An expected advantage of GPPL is that it provides more meaningful uncertainty estimates for tasks such as active learning. 
We examined whether erroneous classifications correspond to more uncertain predictions
when using GPPL and SVM when both methods use the \emph{ling} features.
For UKPConvArgStrict, the mean Shannon entropy
of the pairwise predictions from GPPL 
was .129 for correct predictions and 2.443 for errors,
while for SVM, the mean Shannon entropy was  .188 for correct predictions and 
1.583 for incorrect.
With both methods, more uncertain predictions correlate with more errors,
but the more extreme values for GPPL suggest that its output probabilities more 
accurately reflect the probability of error than those given by the SVM.

% % the analysis below is possible only using the predictions for the training points; however we do not predict for
% % the provided training points?
% To judge whether the GPPL improvements in UKPConvArgCrowdSample are due to better
% handling of noisy labels, we examined whether annotations that disagreed with the 
% MACE gold label -- i.e. erroneous training labels -- led to erroneous predictions. 
% We chart the results of this error analysis in Table \ref{tab:noise_errors}.
% This shows that while arguments with erroneous training labels are more likely to 
% be incorrectly classified using both SVM and GPPL, the Bayesian approach is less susceptible
% to this error.
% \begin{table}
% \begin{tabularx}{\columnwidth}{| l | X | X | X | X | }
% \hline
% & \multicolumn{4}{c | }{\emph{Predictions}} \\
% & \multicolumn{2}{c|}{SVM} & \multicolumn{2}{c|}{GPPL} \\\hline
% \emph{Annotations}           & \emph{True} & \emph{False} & \emph{True} & \emph{False} \\
% Incorrect & & & &  \\
% Correct   & & & & \\
% \hline
% \end{tabularx}
% \caption{Coincidence of erroneous annotations and false predictions by pairwise classification methods.}
% \label{tab:noise_errors}
% \end{table}
