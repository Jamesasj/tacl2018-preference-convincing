\section{Experiments}\label{sec:expts}

The first dataset contains a number of pairwise convincingness preference labels for a set of arguments from a crowd of workers. Each label is associated with two arguments and expresses whether a worker in the crowd found the first argument most convincing, the second argument, or had no preference.  

The task is to train the models then predict the preference labels for held-out data. Each method can be assessed in terms of classification accuracy, since the labels have three possible values. 
\subsection{Hypotheses -- needs to be reconciled with the above paragraph} 

Prior work on convincingness:
\begin{itemize}
 \item \cite{habernal2016argument} shows how to predict convincingness of arguments by training a NN 
 from crowdsourced annotations. 
 \item \cite{lukin2017argument} shows that persuasion is correlated with personality traits.
\end{itemize}

We build on this to show...
\begin{itemize}
 \item How we can rank arguments in terms of convincingness using preference learning, and resolve
 conflicts.
 \item Which input features are informative and how we can learn this using a Bayesian approach
 \item Uncertainty is modelled correctly in the Bayesian approach so that (a) we can filter
 out the uncertain decisions to improve accuracy; (b) the brier score/cross entropy error is lower;
 \item Uncertainty also means that simple active learning works better
 \item Bayesian approach means accuracy is better with sparse data, e.g. in the cold-start situation,
  esp. if we can use (a) and (c) to avoid acting on uncertain labels.
\end{itemize}
This is all useful because we can use the approach to determine which features are worth 
obtaining, make predictions when data is sparse, and obtain data from users efficiently.

Experiment story:
\begin{enumerate}
  \item Compare GP preference learning (GPPL) to SVM using linguistic features. Result: better performance on ranking tasks.
  \item Compare GPPL to Bi-LSTM. Result: GPPL is not as effective when using mean embeddings, but GPPL with linguistic features still outperforms Bi-LSTM on all metrics.
  \item Do the embeddings and linguistic features provide complementary information? Run GPPL with both sets of features. Result: a small improvement on classification tasks, and larger improvement on ranking suggests that the feature sets contain complementary information. The computational cost (of kernel computation, which dominates the overall cost in these experiments) grows linearly with number of features. 
  \item How much does performance drop when we allow conflicts in the preference graph? Compare GPPL, SVM, Bi-LSTM. Result: all methods have a small drop in performance, but GPPL is affected least.
  \item How much does performance drop if we use noisy crowdsourced labels, rather than the gold standard produced by MACE? Compare GPPL, SVM, Bi-LSTM. Result: as in previous experiment, GPPL copes best with the added noise. 
  \item Does GPPL improve ranking performance because of the way it resolves conflicting preferences, or the way it makes predictions? Compare against feature-free preference learning used to train an SVM regression model (PL+SVR). Result: GPPL improves classification slightly on noise-free dataset, and improves more over PL+SVR on ranking and noisy classification tasks. GPPL resolves conflicts at the same time as predicting scores using similarities between arguments in feature space, so therefore has more information to resolve erroneous labels than the feature-free PL.
  \item GP methods are heavily affected by choice of kernel. The standard approach (using a product of kernels for each feature) is equivalent to taking the euclidean distance between points in feature space. These distances become very large when we have a large number of features. Each point needs to be close in all dimensions in order to be close overall. An alternative to this product kernel is to use a sum kernel, which will result in points having strong covariance if some (rather than all) features are similar. This may be suitable for high-dimensional settings where some features may have missing values. Compare the GPPL approach with product and sum kernels. Result: *needs rerunning due to bug in sum kernel*
  \item Besides the choice of kernel itself, another important set of hyperparameters of the GPPL are the hyperparameters for the prior over the output scale of the kernel. The output scale controls the noise of the preferences, and needs to be large enough to allow the posterior to deviate substantially from the prior mean given only a small number of observations at each point. We test three different plausible settings of this hyperparameter to determine how sensitive the results are. Result: heavily informative values do not work well; very noninformative values are effective, although we observe a small boost when using an intermediate setting. This may be due to the data sparsity; the intermediate setting puts more weight onto each individual observation.
  
\end{enumerate}

\begin{table*}
  \begin{tabularx}{\textwidth}{ l  X  X  X }
  Dataset & Dataset properties & Hypothesis & Methods \\
  \hline\hline\\
  1. UKPConvArgStrict & 
  MACE output with confidence $\ge 95\%$; \newline
  Discard arguments marked as equally convincing; \newline
  Discard conflicting preferences. & 
  Bayesian method is competitive with previous methods at predicting clean preference pairs from a clean dataset. &
  GP Preference learning + linguistic features + embeddings. \\\hline\\
  2. UKPConvArgAll & 
  MACE output with confidence $\ge 95\%$;\newline
  No further filtering. & 
  Bayesian method is competitive with previous methods if filtering step is removed. &
  GP Preference learning + linguistic features + embeddings. \\\hline\\      
  3. UKPConvArgRank & 
  MACE output with confidence $\ge 95\%$;\newline
  Equal arguments included; \newline
  PageRank used to rank arguments. & 
  Bayesian method is competitive with previous methods at ranking arguments and 
   can perform ranking given pairs rather than rank scores. &
  GP regression with preference learning output + linguistic features + embeddings (trained on rank scores); \newline
  GP Preference learning + linguistic features + embeddings (trained on pairs). \\\hline\\  
  4. UKPConvArgCrowd & 
  No filtering, all pairs from original workers are provided. & 
  Bayesian method can predict argument pairs for individual annotators with competitive performance to rival methods on clean, combined data; \newline
  There are patterns of common agreement/disagreement among workers. &
  Bayesian Preference Components + linguistic features + embeddings (pair prediction). \\\hline\\
  5. UKPConvArgCrowdR & 
  No filtering, all pairs from original workers are provided; \newline 
  PageRank used to produce gold-standard ranking. & 
  Bayesian model can predict individual argument rankings;\newline
  Significant differences between individual rankings and gold-standard ranking. &
  Bayesian Preference Components + linguistic features + embeddings (ranking output). \\\hline\\ 
  %UKPConvArgAll (train), UKPConvArgStrict (test) &
  %Use the original pairs for training and try to predict the cleaned output &
  %(Optional -- more about preference learning from noisy data, so probably deserves a separate paper/student project?)
  %Show that the model can also be used to predict a gold standard -- end-to-end preference learning &
  %Bayesian Preference Components + linguistic features + embeddings + gold-standard pairs on the training data (model is not really designed for aggregation, but for sharing information between similar people and items; without gold standard training labels, we need to know the y-feature mixture of the gold-standard); 
  %HeatmapBCC with preference learning forward model (deals directly with noisy labels, rather than different opinions about convincingness; useful for learning from implicit feedback)
  \end{tabularx}
  \caption{\label{tab:expt_data} The datasets and hypotheses in each experiment.}
\end{table*}

\subsection{Alternative Application -- Notes}

The core contribution is to do preference learning with sparse observations with text data. 
There may be several other problems related to NLP and argumentation, more specifically, that
could benefit from this approach. 
Argument cloze task? 
The model can be adapted to classification problems, regression, or mixed observation types by applying a different likelihood. The core of the method is the abstraction of a latent function over items and people, dependent on latent features of items and people, with the ability to include side information.
The paper could therefore apply the model to multiple NLP tasks, and be a methodology paper. 
This needs us to discuss and distinguish the class of problems and novelty of the method. 
What are the alternative methods, e.g. if we were to use this for classification? One could supply all 
item and person data to a neural network and train it in a semi-supervised manner?
Sticking to the idea of preference learning or ranking, what other tasks could be handled in this way?
