\section{Introduction}\label{sec:intro}

% Possibly useful phrasing: https://arxiv.org/abs/1707.08349
% The goal of this paper is to demonstrate that our shallow and simple approach based on ... (with minor 
% improvements) can pass the test of time and reach state-of-the-art performance in ...

% Finding well-written and convincing arguments from large bodies of text
%   could enable better decision-making and analysis of controversial topics.
% However, sources of information may vary from social media posts to
%   articles and books, and the language used in each new topic can vary substantially.
%   This presents a challenge when training machine learning algorithms to identify convincing
%   arguments, since annotated data is in short supply. 
% Pairwise preference judgements provide a convenient way for multiple people to communicate the relative convincingness of arguments.
% Implicit preferences can be elicited from user actions, such as selecting a document from a list given its summary to read in more detail. 

Argumentation is intended to persuade the reader of a particular point of view and 
is an important way for humans to reason about controversial topics~\cite{mercier2011humans}. 
The amount of argumentative text on any given topic can, however, overwhelm a reader, particularly
considering the scale of historical text archives 
and the prevalence of social media platforms with millions of authors.
To gain an understanding of a topic, it is therefore useful to identify high-quality, 
persuasive arguments from different sides of a debate.

% Motivations for modelling argument convincingness:
% \begin{itemize}
%   \item Learning about a controversial topic often requires reading large amounts of text, often with much duplicate information, in order to understand different points of view
%   \item Points of view on controversial topics are often presented as arguments for or against a particular position
%   \item Finding well-written arguments could allow better understanding of why people hold particular opinions
%   \item Identifying arguments that are considered convincing to a particular group of people helps understand who holds which point of view
%   \item Tools that identify convincing arguments could therefore assist in making better decisions and analysing public opinion
% \end{itemize}

Previously, Habernal and Gurevych~\shortcite{habernal2016argument} showed that it is possible to predict  the
convincingness of arguments taken from online discussion forums % with reasonable accuracy,
using models trained on one topic and transferred to another.
In the absence of gold-standard ratings for the arguments,  
they crowdsourced pairwise preference labels indicating 
which argument in a pair the annotator found more convincing. 
As a means of eliciting convincingness, pairwise preferences have a number of advantages. 
Firstly, unlike ratings or scores, they do not require calibrating when annotators 
are biased toward high, low or middling scores.
Pairwise comparisons are also more fine-grained than categorical labels 
and can lead to more reliable results with less cognitive burden on human annotators
\cite{kendall1948rank,kingsley2006preference}. 
Implicit preferences can also be elicited from user actions, 
such as selecting a document from a list of summaries~\cite{joachims2002optimizing}.
  
In practice, however, preference data may be noisy, particularly if obtained from crowds or implicit feedback.
We may also be faced with very small amounts of data when we move to new domains and topics,
%topics and users for whom we wish to predict convincingness. 
which can present a problem to methods such as deep neural networks~\cite{srivastava2014dropout}.
The approach used by Habernal and Gurevych \shortcite{habernal2016argument} to handle unreliable 
crowdsourced data involved first determining consensus labels using the MACE algorithm~\cite{hovy2013learning};
these consensus labels were then used to train a classifier and as input to 
PageRank; the resulting rankings were then used as training data for regression models.
However, such pipeline approaches can be prone to error propagation~\cite{chen2016joint} 
and consensus algorithms such as MACE require multiple crowdsourced labels for each argument pair. 

% \begin{itemize}
%   \item New types of text, new domains, and new users with different preferences means we may face situations in practice where models trained on existing corpora are less effective, but data for the new task is limited (sparse)
%   \item Use two different pipelines consisting of multiple steps: combining crowdsourced data, removing inconsistencies, classification; combining crowdsourced data, ranking using PageRank, regression
%   \item In low-data situations, these approaches may underperform, since model uncertainty is not accounted for between each stage, nor in the final predictions, and errors propagate along the pipeline
%   \item Training data may also contain errors, which would be propagated through the pipeline (this was avoided in previous work by combining labels from multiple crowdworkers; we should be able to handle the case where this is not possible).
%   \item Feature space becomes very large when working with textual features -- can we narrow it down automatically to improve scalability and improve performance?
% \end{itemize}

In contrast to previous work, we propose the use of preference learning techniques for argument convincingness
to directly model the relationship between crowdsourced pairwise preferences and textual features, 
including word embeddings.
We choose a Bayesian approach, since Bayesian methods have been 
to handle the problem of small datasets (e.g.~\cite{xiong2011bayesian,titov2012bayesian}), 
unreliable data (e.g.~\cite{simpson2015language}),
and provide a good basis for active selection to reduce labelling costs~\cite{mackay1992information}.
Our method is based on the model of~\cite{chu2005preference},
which assumes that preferences over items are described by a latent preference function.
By providing a Gaussian process (GP) prior over this latent function, 
their method handles uncertainty in the function values due to noise and data sparsity in a principled manner.
This approach has not previously been applied to text problems with large
numbers of features and their proposed inference scheme was limited 
by a computational complexity of $\mathcal{O}(N^3)$, where $N$ is the number of items.
We address the problem of scalability by using recent advances in stochastic variational
inference (SVI)~\cite{hoffman2013stochastic} to develop a new inference approach for Gaussian process preference learning. 
We further show how we our method enables efficient optimisation of important hyper-parameters.
We then demonstrate how our method can be applied to argument convincingness 
with a large number of linguistic features and high-dimensional text embeddings.
%   \item Confidence estimates from Bayesian models account for sparsity and noise in data, as well as uncertainty in the model. This means they do not make overly-confident predictions when training data is small (they know when they don't know).
%  \item Bayesian preference learning methods have been proposed but scalable implementations were not developed and models have not been applied to text with large numbers of features
%  \item We address the limitations above by adapting Bayesian preference learning approach to argumentation
%  \item Introduce stochastic variational inference (SVI) to train the model on large numbers of preferences and documents
%  \item Develop gradient-based ARD to identify relevant text features
%\end{itemize}
Our evaluation compares Bayesian preference learning to established SVM and neural network approaches for predicting convincing arguments, and shows that our approach outperforms these alternatives, 
particularly with small and noisy datasets. 
%  \item Show that Bayesian Gaussian process (GP) models are applicable to performing preference learning over text (existing evaluation of GPs for text is very limited, although they have been used extensively with great success in domains such as Physics, finance, Biology. This is possibly because GPs were seen as more difficult to implement and could not be scaled up until recent advances such as SVI)
%  \item Evaluate the ability of each method to handle noisy and sparse data, showing improved performance using our method in the presence of noise and data sparsity
 % \item Analyse the features that are most informative when determining convincingness, providing insight into what makes a convincing argument
%\end{itemize}

The rest of the paper is structured as follows.
First, we review related work in more detail: on argumentation; Bayesian methods for preference learning; and scalable approximate inference.
Section \ref{sec:model} then details the preference learning model, our SVI inference technique, and hyper-parameter optimisation method.
Section \ref{sec:expts} then presents a number of experiments: a comparison with the
state-of-the art on predicting preferences in online debates; 
noisy datasets; active learning; and feature relevance determination.
Finally, we present conclusions and avenues for future work.

\section{Related Work}\label{sec:related}

%Recent work on argumentation by \cite{habernal2016argument} has established datasets and methods for
%predicting which argument is most convincing. Our experiments make extensive use of this data to establish
%a different methodology. 
Recently, Habernal and Gurevych~\shortcite{habernal2016makes} analysed reasons provided by annotators for why one argument is more convincing than another. In this paper we assume that explicit reasons are not provided. 
Investigations by Lukin et. al~\shortcite{lukin2017argument} demonstrated the effect of personality and the audience's prior stance on persuasiveness,
although their work does not extend to modelling persuasiveness using preference learning.
The sequence of arguments in a dialogue is another important factor in their ability to change
the audience's opinions~\cite{tan2016winning}. Reinforcement learning has been used 
to choose the best argument to present to a user~\cite{rosenfeld2016providing,monteserin2013reinforcement},
but such approaches do not model user preferences for arguments with certain qualities.
 
% GP-based approach versus Bradley/Terry/Luce/MPM method or Mallows models?
The goal of learning an audience's preferences is to rank or score items or 
predict which item in a subset the user prefers. 
A preference for item $x_i$ over $x_j$ is written as $x_i \succ x_j$.
%Given a ranking over items, it is possible to determine the pairwise preferences, but 
Pairwise labels can be predicted using a generic classifier without the need to learn a total ordering.
To do this, pairs of items are transformed either by concatenating the feature vectors of two items~\cite{habernal2016argument}, 
or by computing the difference of the two feature vectors, as in SVM-Rank~\cite{joachims2002optimizing}. 
The classifier is then trained using the transformed feature vectors as input data and 
the preference labels binary class labels.
However, the ranking of items is useful for producing ordered lists in response to a query -- 
consider a sorted list of the most convincing arguments in favour of topic X.
Another approach is to learn the ordering directly using Mallows models~\cite{mallows1957non},
which define distributions over permutations of a list. 
Mallows models have been extended to provide a generative model~\cite{qin2010new} and 
to be trained from pairwise preferences %rather than by observing rankings
~\cite{lu2011learning}, but inference is typically costly
since the number of possible permutations to be considered is $\mathcal{O}(N^2)$, 
where $N$ is the number of items to be ranked. 
Modelling only the order of items means we are unable to quantify 
how closely rated items at similar ranks are to one another: how much better is the top ranked item 
from the second-rated?

% Bradley-Terry: MPM\cite{volkovs_new_2014}
To avoid the problems of classifier-based and permutation-based methods, 
another approach is to learn a set of real-valued scores from pairwise labels
that can be used to predict rankings, pairwise labels, or as ratings for individual items.
There are two established approaches for mapping discrete pairwise labels to real-valued scores:
the Bradley-Terry-Plackett-Luce model~\cite{bradley1952rank,luce1959possible,plackett1975analysis}
and the Thurstone-Mosteller model~\cite{thurstone1927law,mosteller2006remarks}.
More recently, Bayesian extensions of the Bradley-Terry-Plackett-Luce model
were proposed by~\cite{guiver2009bayesian,volkovs_new_2014}, 
while the Thurstone-Mosteller model was used by~\cite{chu2005preference}.
This latter piece of work assumes a Gaussian process (GP) prior over the scores,
which enables us to predict scores for previously unseen items given their features 
using a Bayesian nonparametric approach.
Nonparametric methods allow the function complexity to grow with the amount of data. 
Gaussian processes are a well established tool for extrapolating from training data 
in a principled manner, taking into account model uncertainty that may arise when 
data for new domains is limited~\cite{rasmussen_gaussian_2006}.
%These characteristics make them suitable for the task of modelling argument convincingness
%where data for new topics, domains and users is limited. 

% The Gaussian process (GP) preference learning approach of \cite{chu2005preference} resolves inconsistencies between preferences and provides a way to predict rankings or preferences for 
% items for which we have not observed any pairwise comparisons based on the item's features. 
% This model assumes that preferences are noisy, i.e. contain some erroneous labels.
% particularly as the modular nature of inference algorithms such as Gibb's sampling and variational approximation is suited to extending the model to handle different types of feedback that give indications of some underlying preferences. 

The inference method used by Chu and Ghahramani~\shortcite{chu2005preference}% used the Laplace approximation to perform approximate
%inference over the model. Unfortunately 
has memory and computational costs that scale with $\mathcal{O}(N^3)$. %due to matrix inversion. 
Besides this limitation, there is also a computational and memory cost 
during training of $\mathcal{O}(N^2)$ due to the number of pairs in the training dataset.
Recently, the stochastic variational inference (SVI) algorithm proposed by \cite{hoffman2013stochastic} 
has been used to address this problem in Gaussian process models~\cite{hensman2013gaussian,hensman_scalable_2015} 
but has not previously been adapted for preference learning with GPs.
% The modular nature of VB allows us to take advantage of models for feedback of different types
% where the input values for each type of feedback do not directly correspond (e.g. explicit user ratings and number of clicks may have different values).
% By using SVI, we provide a formal way to deal with scalability that comes with guarantees\cite{hoffman2013stochastic}.
The next section explains how we apply this technique to create a scalable preference learning method for argument convincingness.

%GPs for NLP in other task areas?
