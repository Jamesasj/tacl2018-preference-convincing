\section{Scalable Bayesian Preference Learning}\label{sec:model}

Following Chu and Ghahramani~\shortcite{chu2005preference}, 
we model the relationship between a latent convincingness function, $f$,
and each observed pairwise label, $v_k \succ u_k$, where $k$ is an index into a list of 
$P$ pairs, as follows:
\begin{flalign}
& p( v_k \succ u_k | f(v_k), f(u_k), \delta_{v_k}, \delta_{u_k} ) & \nonumber\\
& \hspace{0.9cm} = \begin{cases}
 1 & \text{if }f(v_k) + \delta_{v_k} \geq f(u_k) + \delta_{u_k} \\
 0 & \text{otherwise,}
 \end{cases} &
 \label{eq:pl}
\end{flalign}
where $\delta_i \sim \mathcal{N}(0, 1)$ is Gaussian-distributed noise. 
The noise term allows for variations in the observed preferences, which may occur if 
different annotators disagree or change their minds, or if
the preferences are derived from noisy implicit data such as clicks streams.
We assume a Gaussian process prior, $f \sim \mathcal{GP}(0, k_{\theta}/s)$, where 
$k_{\theta}$ is a kernel function with hyper-parameters $\theta$, 
and $s \sim \mathcal{G}(a_0, b_0)$ is an inverse scale parameter 
drawn from a gamma prior with shape $a_0$ and scale $b_0$.
%The kernel function controls the smoothness of $f$ over the feature space.

The inference goal is to learn the posterior distribution over the function values $f(\mathbf{x})$ for each 
argument feature vector $\mathbf{x}$.
Chu and Ghahramani~\shortcite{chu2005preference}
used a Laplace approximation, which finds a maximum a-posteriori (MAP) solution
that has been shown to perform poorly in many cases
~\cite{nickisch2008approximations}. 
Instead, we use a variational approximation to a fully Bayesian approach
~\cite{reece2011determining,steinberg2014extended} and adapt this method
to the preference likelihood given by Equation \ref{eq:pl}.
Given a set of observed preference pairs, $\bs y$,
we assume an approximation, $q(f,s)$,  to the true posterior distribution, $p(f,s|\bs y, \theta, a_0, b_0)$.
We then update $q(f,s)$ iteratively to maximises a lower bound on the log marginal likelihood, 
$\mathcal{L} \leq \log p(\bs y | \theta, a_0, b_0)$.
This optimisation procedure minimises the Kullback-Leibler divergence of $p(f,s|\bs y, \theta, a_0, b_0)$ from $q(f,s)$,
meaning that $q(f,s)$ converges to an approximate posterior. 

The variational approach still requires an $\mathcal{O}(N^3)$ matrix inversion,
which is impractical with more than a few hundred data points. 
However, the recent introduction of stochastic variational inference (SVI) 
\cite{hoffman2013stochastic,hensman_scalable_2015}
 means we can adapt our variational inference method
to scale to datasets containing at least tens of thousands of 
arguments and pairwise labels.

SVI assumes $M$ \emph{inducing points},
which act as a substitute for the observed arguments,
and considers only a random subset of the data containing $P_n$ pairs at each iteration. 
By choosing $M << N$ and $P_n << P$, we limit the computational
complexity to $\mathcal{O}(M^3 + MP_n)$ and the 
memory complexity $\mathcal{O}(M^2 + MP_n + P_n^2)$.
To choose representative inducing points, 
we use K-means with $K=M$ to rapidly cluster the feature vectors, 
then take the cluster centres as inducing points.

A further benefit of GPs is that they enable automatic relevance determination (ARD)
to identify informative features, which works as follows.
The prior covariance of $f$ is defined by a kernel function of the form 
$k_{\theta}(\bs x, \bs x') = \prod_{d=1}^D k_d(|x_d - x_d'| / l_d)$, 
where $k_d$ is a function of the distance between the values of feature $d$ 
for items $x$ and $x'$, and a length-scale hyper-parameter, $l_d$.
The length-scale controls the smoothness of the function across the feature space,
and can be optimised by choosing the value of $l_d$ that maximises the lower bound on 
the log marginal likelihood, $\mathcal{L}$. 
This process is known as maximum likelihood II~\cite{rasmussen_gaussian_2006}.
Features with larger length-scales after optimisation are less relevant because their values
have less effect on $k_{\theta}(\bs x, \bs x') $.
To cut out the cost of optimising the length-scales, we can also use a median heuristic,
which has been shown to perform well in practice~\cite{gretton2012optimal}: 
$ l_{d,MH} = \frac{1}{D} \mathrm{median}( \{ |x_{i,d} - x_{j,d}| \forall i=1,..,N, \forall j=1,...,N\} ) $.
