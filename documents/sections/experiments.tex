\section{Experiments}\label{sec:expts}

The first dataset contains a number of pairwise convincingness preference labels for a set of arguments from a crowd of workers. Each label is associated with two arguments and expresses whether a worker in the crowd found the first argument most convincing, the second argument, or had no preference.  

The task is to train the models then predict the preference labels for held-out data. Each method can be assessed in terms of classification accuracy, since the labels have three possible values. In the first set of experiments we evaluate the baselines and the different methods for modelling correlations between workers' preferences. In the second set of experiments, we assess the value of different language features. Finally, the third experiment evaluates approaches that integrate both argument features and models of preference correlations.

\subsection{Hypotheses -- needs to be reconciled with the above paragraph} 

Prior work on convincingness:
\begin{itemize}
 \item \cite{habernal2016argument} shows how to predict convincingness of arguments by training a NN 
 from crowdsourced annotations. 
 \item \cite{lukin2017argument} shows that persuasion is correlated with personality traits.
\end{itemize}

We build on this to show...
\begin{itemize}
 \item How we can predict convincingness for a specific user given only previous preferences and 
 preferences of others (collaborative filtering)
 \item How we can rank arguments in terms of convincingness using preference learning, and resolve
 conflicts.
 \item How a combination of text and personality features improves predictions of convincingness
 \item Which input features/side information features are informative and how we can learn this using a Bayesian approach
 \item That we can extract human-interpretable latent features in people and items,
 which improve performance over just using the input features.
 \item Uncertainty is modelled correctly in the Bayesian approach so that (a) we can filter
 out the uncertain decisions to improve accuracy; (b) the brier score/cross entropy error is lower;
 \item Uncertainty also means that simple active learning works better
 \item Bayesian approach means accuracy is better with sparse data, e.g. in the cold-start situation,
  esp. if we can use (a) and (c) to avoid acting on uncertain labels.
\end{itemize}
This is all useful because we can use the approach to determine which features are worth 
obtaining, make predictions when data is sparse, and obtain data from users efficiently.

The steps to show this are:
\begin{enumerate}
  \item Show a table comparing the baselines, alternative collaborative filtering methods, 
  results from \cite{habernal2016argument}, and unsupervised method
  \item Add in results when using the input information with out method
  \item Show a table comparing the baselines, alternative collaborative filtering methods, 
  results from \cite{lukin2017argument}, and unsupervised method
  \item Add in results using item information, person information and both
  \item Visualise latent features?
  \item Table showing importance of input features
  \item Add results with lower confidence items excluded to the tables in 1-4. We can also plot the
  effect of confidence threshold on our results and on the rival methods.
  \item Add in Bier/cross entropy -- may need to rerun the original code from the previous papers?
  \item Run \cite{habernal2016argument} and my complete method with reduced data -- check accuracy as it increases. Use confidence cut-off from previous results.
  \item Simple active learning approach selecting the most uncertain data point (this will be due to 
  uncertainty about a person, an item with too little data, or disagreement/stochasticity in the likelihood). The plot can be added to the previous results and should be run with rival methods.
\end{enumerate}

\begin{table*}
  \begin{tabularx}{\textwidth}{ l  X  X  X }
  Dataset & Dataset properties & Hypothesis & Methods \\
  \hline\hline\\
  1. UKPConvArgStrict & 
  MACE output with confidence $\ge 95\%$; \newline
  Discard arguments marked as equally convincing; \newline
  Discard conflicting preferences. & 
  Bayesian method is competitive with previous methods at predicting clean preference pairs from a clean dataset. &
  GP Preference learning + linguistic features + embeddings. \\\hline\\
  2. UKPConvArgMACE & 
  MACE output with confidence $\ge 95\%$;\newline
  No further filtering. & 
  Bayesian method is competitive with previous methods if filtering step is removed. &
  GP Preference learning + linguistic features + embeddings. \\\hline\\      
  3. UKPConvArgRank & 
  MACE output with confidence $\ge 95\%$;\newline
  Equal arguments included; \newline
  PageRank used to rank arguments. & 
  Bayesian method is competitive with previous methods at ranking arguments and 
   can perform ranking given pairs rather than rank scores. &
  GP regression with preference learning output + linguistic features + embeddings (trained on rank scores); \newline
  GP Preference learning + linguistic features + embeddings (trained on pairs). \\\hline\\  
  4. UKPConvArgAll & 
  No filtering, all pairs from original workers are provided. & 
  Bayesian method can predict argument pairs for individual annotators with competitive performance to rival methods on clean, combined data; \newline
  There are patterns of common agreement/disagreement among workers. &
  Bayesian Preference Components + linguistic features + embeddings (pair prediction). \\\hline\\
  5. UKPConvArgAllR & 
  No filtering, all pairs from original workers are provided; \newline 
  PageRank used to produce gold-standard ranking. & 
  Bayesian model can predict individual argument rankings;\newline
  Significant differences between individual rankings and gold-standard ranking. &
  Bayesian Preference Components + linguistic features + embeddings (ranking output). \\\hline\\ 
  %UKPConvArgAll (train), UKPConvArgStrict (test) &
  %Use the original pairs for training and try to predict the cleaned output &
  %(Optional -- more about preference learning from noisy data, so probably deserves a separate paper/student project?)
  %Show that the model can also be used to predict a gold standard -- end-to-end preference learning &
  %Bayesian Preference Components + linguistic features + embeddings + gold-standard pairs on the training data (model is not really designed for aggregation, but for sharing information between similar people and items; without gold standard training labels, we need to know the y-feature mixture of the gold-standard); 
  %HeatmapBCC with preference learning forward model (deals directly with noisy labels, rather than different opinions about convincingness; useful for learning from implicit feedback)
  \end{tabularx}
  \caption{\label{tab:expt_data} The datasets and hypotheses in each experiment.}
\end{table*}

\subsection{Alternative Application -- Notes}

The core contribution is to do preference learning with sparse observations with text data. 
There may be several other problems related to NLP and argumentation, more specifically, that
could benefit from this approach. 
Argument cloze task? 
The model can be adapted to classification problems, regression, or mixed observation types by applying a different likelihood. The core of the method is the abstraction of a latent function over items and people, dependent on latent features of items and people, with the ability to include side information.
The paper could therefore apply the model to multiple NLP tasks, and be a methodology paper. 
This needs us to discuss and distinguish the class of problems and novelty of the method. 
What are the alternative methods, e.g. if we were to use this for classification? One could supply all 
item and person data to a neural network and train it in a semi-supervised manner?
Sticking to the idea of preference learning or ranking, what other tasks could be handled in this way?
