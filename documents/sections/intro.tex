\section{Introduction}\label{sec:intro}

We investigate whether certain combinations of textual features are indicative of an argument's convincingness.
We hypothesise that different people find different types of argument more convincing than others and therefore, textual features have varying levels of importance in determining convincingness, depending on the audience. 
We propose to model the relationships between convincingness and textual features to predict which arguments a person will prefer.
We hypothesise that these predictions will be more accurate if we adapt the model to the individual reader based on their previously observed preferences. 
However, individual preference data is likely to be very sparse, 
and the computational cost of modelling very high, 
meaning that it is impractical to learn independent models for each person.
Our approach is therefore to identify correlations between different people's preferences
so that we can learn shared models of convincingness that can then be adapted to individuals to improve predictions of argument convincingness. 
We aim to establish whether such a model can be learned by observing pairwise convincingness preferences, 
%and whether language features extracted from arguments can further improve performance when which argument a person will find most convincing. 

The experiments evaluate a number of techniques for modelling worker preferences, different types of language features, and the correlations between workers and features. 
We investigate whether workers with similar preferences according to each model give similar justifications for their decisions, thereby lending additional support for models based on correlations between preferences.

We provide a new preference learning model to handle large numbers of potentially very sparse features and large numbers of people. Our Bayesian approach enables us to 
perform automatic feature selection, learn in semi-supervised or unsupervised modes, 
and fully account for model and parameter uncertainty, while scaling to large numbers of input features. 

\section{Related Work}\label{sec:related}

Related work on argumentation and persuasion. Related work on finding reasons for argument convincingness (cite Ivan). Related work on choosing the best argument in sequence \cite{rosenfeld2016providing, monteserin2013reinforcement}. 

Preference learning from pairwise preferences is effective because it removes the need for humans to provide scores or classifications and allows them to make relevance judgements,
which have been shown to be easier for human annotators in many cases\cite{brochu_active_2007}. Pairwise comparisons also occur in implicit feedback, for example, when a user chooses to click on link from a list of several. They are therefore a useful tool for practical learning from end users. 
However, the pairwise comparisons we observe may not be a perfect representation of their preferences as they may contain noise, leading to inconsistencies where items cannot be ranked in such a way that the ranking agrees with all the observed comparisons. 
The Gaussian process (GP) preference learning approach of \cite{chu_preference_2005} resolves such inconsistencies and provides a way to predict rankings or preferences for 
items for which we have not observed any pairwise comparisons based on the item's features. 
An extension to multiple users was proposed by \cite{houlsby2012collaborative}, 
but this method suffered from poor scalability.#

Matrix factorisation techniques are commonly used in recommender systems to discover latent
user and item features but can fail if the
data is very sparse unless suitably regularised or given a Bayesian treatment.
Matrix factorisation techniques are also unsuitable for pairwise comparisons as they 
must be learned using explicit numerical ratings.
A more scalable approach that incorporates probabilistic matrix factorisation
(specifically, probabilistic PCA) was proposed by \cite{khan2014scalable}.
Their method is applicable to both pairwise comparisons and ratings data
and as such could be used to learn the model from implicit feedback such as clicks on an item. However, it may be more suitable to use a model for such feedback that explicitly considers the different bias and noise of each type or source of feedback. For such
a purpose, the model of \cite{dawid_maximum_1979} may be appropriate but has to date
been used for classifier combination and categorical labelling tasks in crowdsourcing and has not been applied to preference learning from different types of feedback. 
Bayesian approaches are suited to handling these problems of data sparsity, noise and bias, 
particularly as the modular nature of inference algorithms such as Gibb's sampling and variational approximation is suited to extending the model to handle different types of feedback that give indications of some underlying preferences. 

The GP methods require $\mathcal{O}(P_n)$ steps, where $P_n$ is the number of pairs for 
user $n$. 
The method proposed by \cite{khan2014scalable} reduces this scaling issue by using a random sample of pairs at each iteration of their EM algorithm.
We use SVI to address scalability in a variational Bayesian framework. 
The modular nature of VB allows us to take advantage of models for feedback of different types
where the input values for each type of feedback do not directly correspond (e.g. explicit user ratings and number of clicks may have different values).
By using SVI, we provide a formal way to deal with scalability that comes with guarantees\cite{SVI paper}.
We also estimate the output scale of the GPs, the latent factors, and item bias as part of the 
variational approximation. %not clear what the true advantage of this is?


We compare our work on Sushi-A dataset or against the method of \cite{khan14scalable} to see if 
our modifications are actually useful. 

Factor analysis differs from PPCA in allowing only diagonal noise covariance matrices, making 
the observed variables conditionally independent given the latent variables. It also provides
a probabilistic treatment for inferring the latent features. %are we still using FA?

We also investigate whether argumentation preferences can be reduced to a simpler
clustering structure, which may be easier to learn with very sparse user data.

% possible extension: state variable to describe what was previously seen? This could relate to time 
% since argument seen, and can be converted to an input feature for the GP model: exp(-t). I think
% that learning length scale and output scale for this feature would work.
