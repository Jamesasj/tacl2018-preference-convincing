\section{Identifying Common Patterns of Convincingness}\label{sec:model}

\subsection{Baseline methods}

\begin{itemize}
  \item Random: select a label at random
  \item Most common (MC): select the most common preference label from across the dataset
  \item No differentiation (ND): we do not model differences between workers. Labels are estimated by taking the average of other people's labels for the same preference pair. When there are no previous pairs available, select the most common preference label
  \item Gaussian process preference learning with no differentiation (GP-ND): learn a latent ranking function for the objects from pairwise preferences, ignoring differences between workers and features of the arguments. This provides a probabilistic variant of ND  
\end{itemize}

\subsection{Modelling Correlations Between Individuals}

Two main types of approach:
\begin{itemize}
  \item Factor analysis -- map the set of pairwise preferences to a low-dimensional embedding
  \item Clustering -- assumes that people fall into distinct preference clusters, or can be modelled as a mixture of several archetypes
\end{itemize}

Specific methods to test can be split into several types. First,
we can run different clustering methods on the training data, 
then predict a worker's label by taking the mean of the other cluster members. 
When the no members of the cluster have labelled the pair, we predict using the most common label.
This method is applied to several clustering algorithms:
\begin{itemize}
   \item Affinity propagation (AP-mean)
   \item Gaussian mixture model, using most probable cluster assignment (GMM-mean)
   \item Gaussian mixture model, using cluster assignments weighted by probability (GMM-WM)
\end{itemize}

A similar approach can be taken with dimensionality reduction techniques, where we can use K-nearest neightbours (in this case, few workers label each pair, so we choose k=1 and use MC when no workers have labelled the current instance?):
\begin{itemize}
   \item Factor analysis with K-nearest neighbours (FA-KNN)
\end{itemize}
Alternatively, we can take a weighted average of the other labels for a pair, where the weights are based on inverse distance from the worker in question in the embedded space:
\begin{itemize}
   \item Factor analysis with an inverse distance-weighted mean (FA-weighted)
\end{itemize}
The distance function can be optimised, which leads to proposing more sophisticated methods...

\section{Bayesian Preference Learning Model}

The model introduced in \cite{houlsby2012collaborative} combines preference learning with matrix factorisation 
to identify latent features of items and users that affect their preferences. This allows for a collaborative filtering effect, whereby users with similar preferences on a set of observed items are assumed to have similar 
preferences for other items with similar features. This allows us to make better predictions about the unobserved preferences of a given user when we have seen preferences of a similar user.

The method presented in \cite{houlsby2012collaborative} uses a combination of expectation propagation (EP) and variational Bayes (VB). Since the inference steps require inverting a covariance matrix, this method scales with 
$\mathcal{O}(N^3)$ and is therefore impractical for large datasets. For our modified version of this method, we improve scalability by using stochastic variational inference to infer the complete model. 
The variational approximation to the posterior is given by...

The variational inference algorithm maximises a lower bound on the log marginal likelihood:
\begin{flalign*}
  \mathcal{L} = \sum_{i=1}^N \mathbb{E}[ \log p(t_i | x_{i,1}, x_{i,2}, \bs f) ] + \nonumber\\
  \sum_{u=1}^U  \mathbb{E}\left[ \log \frac{p(\bs f_u | \bs w \bs y_u, \bs K_{f,u} / s_{f,u} )}{q(\bs f_u)}\right] + \nonumber\\
  \sum_{c=1}^C \mathbb{E}\left[\log\frac{p(\bs w_c | \bs 0, \bs K_w / s_{w,c} ) }{q(\bs w_c) } \right] + \nonumber\\
  \sum_{c=1}^C \mathbb{E}\left[\log\frac{p(\bs y_c | \bs 0, \bs K_y / s_{y,c} ) }{q(\bs y_c) } \right] + \nonumber\\
  \mathbb{E}\left[\log\frac{p(\bs t | \bs \mu, \bs K_{t} / s_t ) }{q(\bs t)} \right] + \nonumber\\
  \sum_{u=1}^U \mathbb{E}\left[\log\frac{p(s_{f,u} | a_{f,u}, b_{f,u})}{q(s_{f,u})}\right] + \nonumber\\
  \sum_{d=1}^D \mathbb{E}\left[\log\frac{p(s_{w,d} | a_{w,d}, b_{w,d})}{q(s_{w,d})}\right] +\nonumber\\
  \sum_{d=1}^D \mathbb{E}\left[\log\frac{p(s_{y,d} | a_{y,d}, b_{y,d})}{q(s_{y,d})}\right] 
\end{flalign*}
where $t_i$ is the preference label for the $i$th pair, 

To perform feature selection with large numbers of features, we introduce 
an automatic relevance determination
(ARD) approach that uses the gradient of the lower bound on the log marginal likelihood to optimise the kernel length-scales using the conjugate gradient method. The gradient is given by:
\begin{flalign*}
\nabla\mathcal{L} = \left[ \frac{\partial \mathcal{L}}{\partial l_{w,1}}, ...,  \frac{\partial \mathcal{L}}{\partial l_{w,D_w}},  \frac{\partial \mathcal{L}}{\partial l_{y,1}}, ...,  \frac{\partial \mathcal{L}}{\partial l_{y,D_y}} \right], &&\\
\frac{\partial \mathcal{L}}{\partial l_{w,d}} = \frac{\partial}{\partial l_{w,d}} 
\sum_{u=1}^U  \mathbb{E}\left[\log\frac{p(\bs f_u | \bs w \bs y_u, \bs K_{f,u} / s_{f,u} )}{q(\bs f_u)}\right] + \nonumber && \\
\sum_{c=1}^C \mathbb{E}\left[\log\frac{p(\bs w_c | \bs 0, \bs K_w / s_{w,c} ) }{q(\bs w_c) } \right] - \nonumber&&\\
\sum_{u=1}^U \mathbb{E}\left[\log q(s_{f,u})\right] - \sum_{d=1}^D \mathbb{E}\left[\log q(s_{w,d})\right] +\nonumber&&\\
= 0.5 (\hat{f}_u - wy_u)^T \bs K_{f,u}^{-1} \frac{\partial \bs K}{\partial \log l_{w,d}} \hat{s}_{f,u} \bs K_{f,u}^{-1} (\hat{f}_u - wy_u) \nonumber\\
- 0.5\mathrm{tr}\left( (\bs K_{f,u}^{-1} - \frac{\bs C^{-1}}{\hat{s}_{f,u}} ) \frac{\partial \bs K_{f,u}}{\partial \log l_{w,d}} \right)\nonumber&&\\
\frac{\partial \mathcal{L}}{\partial l_{y,d}} = &&\\
\end{flalign*}
where $l_{w,d}$ is a length-scale used for all the GPs over item features. The implicit terms are zero when the VB algorithm has converged.

\pagebreak
No
\pagebreak
